{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f62f2e91",
   "metadata": {},
   "source": [
    "# <i class=\"fa-solid fa-brain\"></i> fMRI Multiverse\n",
    "\n",
    "*Note: You can download this individual file as a Jupyter Notebook (.ipynb) file by clicking the download button at the top.*\n",
    "\n",
    "<iframe width=\"800\" height=\"450\" src=\"../../_static/multiverse_fmri.pdf\" frameborder=\"0\" allowfullscreen></iframe>\n",
    "\n",
    "## Pre-load the data\n",
    "\n",
    "For the fMRI multiverse analysis example, we will use data from 100 participants of the [Autism Brain Imaging Data Exchange (ABIDE)](http://preprocessed-connectomes-project.org/abide/) preprocessed connectomes dataset.\n",
    "\n",
    "- Run the following code cell to download the data from [nilearn](https://nilearn.github.io/dev/modules/generated/nilearn.datasets.fetch_abide_pcp.html) into the `data/` folder. To speed things up, you can also use one of our USB sticks and copy the entire `data/` folder into the directory of this script.\n",
    "\n",
    "- Inspect the following code cell and visit the [website](http://preprocessed-connectomes-project.org/abide/) of the dataset to familiarize yourself with the data. What do the individual options mean, and are they reasonable decisions for a multiverse analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "131daee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:25,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available keys:     [('cpac', True, True, 'rois_aal'), ('cpac', True, True, 'rois_cc200'), ('cpac', True, True, 'rois_dosenbach160'), ('cpac', True, False, 'rois_aal'), ('cpac', True, False, 'rois_cc200'), ('cpac', True, False, 'rois_dosenbach160'), ('cpac', False, True, 'rois_aal'), ('cpac', False, True, 'rois_cc200'), ('cpac', False, True, 'rois_dosenbach160'), ('cpac', False, False, 'rois_aal'), ('cpac', False, False, 'rois_cc200'), ('cpac', False, False, 'rois_dosenbach160'), ('ccs', True, True, 'rois_aal'), ('ccs', True, True, 'rois_cc200'), ('ccs', True, True, 'rois_dosenbach160'), ('ccs', True, False, 'rois_aal'), ('ccs', True, False, 'rois_cc200'), ('ccs', True, False, 'rois_dosenbach160'), ('ccs', False, True, 'rois_aal'), ('ccs', False, True, 'rois_cc200'), ('ccs', False, True, 'rois_dosenbach160'), ('ccs', False, False, 'rois_aal'), ('ccs', False, False, 'rois_cc200'), ('ccs', False, False, 'rois_dosenbach160'), ('dparsf', True, True, 'rois_aal'), ('dparsf', True, True, 'rois_cc200'), ('dparsf', True, True, 'rois_dosenbach160'), ('dparsf', True, False, 'rois_aal'), ('dparsf', True, False, 'rois_cc200'), ('dparsf', True, False, 'rois_dosenbach160'), ('dparsf', False, True, 'rois_aal'), ('dparsf', False, True, 'rois_cc200'), ('dparsf', False, True, 'rois_dosenbach160'), ('dparsf', False, False, 'rois_aal'), ('dparsf', False, False, 'rois_cc200'), ('dparsf', False, False, 'rois_dosenbach160'), ('niak', True, True, 'rois_aal'), ('niak', True, True, 'rois_cc200'), ('niak', True, True, 'rois_dosenbach160'), ('niak', True, False, 'rois_aal'), ('niak', True, False, 'rois_cc200'), ('niak', True, False, 'rois_dosenbach160'), ('niak', False, True, 'rois_aal'), ('niak', False, True, 'rois_cc200'), ('niak', False, True, 'rois_dosenbach160'), ('niak', False, False, 'rois_aal'), ('niak', False, False, 'rois_cc200'), ('niak', False, False, 'rois_dosenbach160')]\n",
      "Number of subjects: 100\n",
      "Class distribution: DX_GROUP\n",
      "1    50\n",
      "2    50\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from nilearn import datasets\n",
    "\n",
    "pipeline = [\"cpac\", \"ccs\", \"dparsf\", \"niak\"]                    # Preprocessing pipelines\n",
    "band_pass = [True, False]                                       # Band-pass filtering   \n",
    "global_signal = [True, False]                                   # Global signal regression \n",
    "parcellation = [\"rois_aal\", \"rois_cc200\", \"rois_dosenbach160\"]  # Parcellated time series data\n",
    "\n",
    "# Download the ABIDE dataset with all combinations of the decision points\n",
    "abide_dataset = {} # store in a dict (only needed to print some information at the end of this cell)\n",
    "for pipe, bp, gsr, parc in tqdm(itertools.product(pipeline, band_pass, global_signal, parcellation)):\n",
    "    bunch = datasets.fetch_abide_pcp(\n",
    "        data_dir=\"./data\", n_subjects=100, quality_checked=True, verbose=0,\n",
    "        pipeline=pipe, derivatives=parc, band_pass_filtering=bp, global_signal_regression=gsr)\n",
    "    abide_dataset[(pipe, bp, gsr, parc)] = bunch\n",
    "\n",
    "print(f\"Available pipelines: {list(abide_dataset.keys())}\")\n",
    "print(f\"Number of subjects:  {len(abide_dataset[('cpac', True, True, 'rois_aal')].phenotypic)}\")\n",
    "print(f\"Class distribution:  {abide_dataset[('cpac', True, True, 'rois_aal')].phenotypic['DX_GROUP'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ff16b5",
   "metadata": {},
   "source": [
    "## Exercise: Create and run the multiverse\n",
    "\n",
    "The multiverse which we will implement is is similar to the one perfomed by [Dafflon et al. 2022](https://www.nature.com/articles/s41467-022-31347-8). The main difference is that we will only apply two connectivity methods and also only use data for 100 participants to reduce memory and cpu load.\n",
    "\n",
    "Please implement the multiverse analysis as outlined in the slides. For the connectivity measure, cou can use the connectivity methods implemented in the Comet toolbox as follows (repplace ``<time_series> with the actual variable name of your time series data)\n",
    "\n",
    "- Pearson correlation (`comet.connectivity.Static_Pearson(<time_series>).estimate()`)\n",
    "- Partial correlation (`comet.connectivity.Static_Partial(<time_series>).estimate()`)\n",
    "\n",
    "The final result should be a specification curve with the following options:\n",
    "\n",
    "`measure = \"accuracy\", height_ratio=(1,1), figsize=(9,7), baseline=0.5, ci=95, p_value=0.05, line_pad=0.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet import multiverse\n",
    "\n",
    "forking_paths = {\n",
    "    ...\n",
    "}\n",
    "\n",
    "def analysis_template():\n",
    "    import comet\n",
    "    import numpy as np\n",
    "    from nilearn import datasets\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "    # Get data (if it is preloaded this will skip the download)\n",
    "    data = datasets.fetch_abide_pcp(data_dir=\"./data\", n_subjects=100, quality_checked=True, verbose=0,\n",
    "                                    pipeline={{pipeline}},\n",
    "                                    derivatives={{parcellation}},\n",
    "                                    band_pass_filtering={{band_pass}},\n",
    "                                    global_signal_regression={{global_signal}})\n",
    "\n",
    "    time_series = ...\n",
    "    diagnosis = ...\n",
    "\n",
    "    # Calculate FC\n",
    "    tri_ix = None\n",
    "    features = []\n",
    "\n",
    "    for ts in time_series:\n",
    "        FC = ...\n",
    "\n",
    "        if tri_ix == None:\n",
    "            tri_ix = np.triu_indices_from(FC, k=1)\n",
    "        \n",
    "        feat_vec = FC[tri_ix]\n",
    "        features.append(feat_vec)\n",
    "\n",
    "    # Prepare features (FC estimates) and target (autism/control)\n",
    "    X = np.vstack(features)\n",
    "    X[np.isnan(X)] = 0.0\n",
    "    y = np.array(diagnosis)\n",
    "\n",
    "    # Classification model\n",
    "    model = Pipeline([('scaler', StandardScaler()), ('reg', LogisticRegression(penalty='l2'))])\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    accuracies = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "\n",
    "    # Save the results\n",
    "    ...\n",
    "\n",
    "# Create and run the multiverse analysis\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ef4df",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "- Discuss the specification curve. How would you interpret the results? What could be the next steps?\n",
    "- Do you see any statistical issues with the classification model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
